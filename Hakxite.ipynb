{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb35CRDODkDPwlO/XWn+5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitsingh09/Hakxite/blob/main/Hakxite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 (Vani)"
      ],
      "metadata": {
        "id": "grhTFtXXcDDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyhdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05noU-Z2OviW",
        "outputId": "075084cb-92ef-4106-b8bb-32b5fce5e6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyhdf\n",
            "  Downloading pyhdf-0.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/771.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/771.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.0/771.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.6/771.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyhdf) (1.25.2)\n",
            "Installing collected packages: pyhdf\n",
            "Successfully installed pyhdf-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np  # For image manipulation\n",
        "from pyhdf import SD  # For reading HDF5 files (install using pip install pyhdf)\n",
        "from copy import deepcopy  # For saving the best model\n",
        "from sklearn.metrics import r2_score  # For R-squared calculation"
      ],
      "metadata": {
        "id": "JOtI_2eHOj5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to load and preprocess Black Marble Night Lights data\n",
        "class NightLightDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform  # Optional data transformation (e.g., normalization)\n",
        "\n",
        "        # Read the HDF5 file\n",
        "        self.hdf_file = SD(data_path, mode='r')\n",
        "        self.night_light_data = self.hdf_file.select('F10')  # Assuming F10 holds night light intensity\n",
        "\n",
        "    def __len__(self):\n",
        "        # Assuming the first dimension of night_light_data represents samples\n",
        "        return len(self.night_light_data[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load night light data for a single sample\n",
        "        night_light_image = self.night_light_data.select(start=(idx, 0), end=(idx, -1))[:, :]\n",
        "\n",
        "        # Preprocessing (replace with your specific steps)\n",
        "        night_light_image = np.array(night_light_image).astype(np.float32)  # Convert to float32\n",
        "        night_light_image = (night_light_image - night_light_image.min()) / (night_light_image.max() - night_light_image.min())  # Normalize to 0-1\n",
        "\n",
        "        if self.transform:\n",
        "            night_light_image = self.transform(night_light_image)  # Apply optional transformations\n",
        "\n",
        "        return night_light_image\n"
      ],
      "metadata": {
        "id": "LOczpyQiOnsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple CNN model architecture (customize based on complexity)\n",
        "class PovertyMapper(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PovertyMapper, self).__init__()\n",
        "        # Adjust filters, kernel sizes, and number of layers as needed\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=7 * 7 * 32, out_features=128)\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer for predicted poverty level\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 7 * 7 * 32)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wBrAE-IbPIdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with validation (R-squared evaluation)\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  # For saving the best model based on validation R-squared\n",
        "    best_val_r2 = float('-inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            # Assuming labels not available, predict poverty level (0 for now)\n",
        "            loss = criterion(outputs, torch.zeros_like(outputs))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Validation (calculate R-squared)\n",
        "        val_loss, val_r2 = evaluate_model(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if val_r2 > best_val_r2:  # Add indentation here (fixed)\n",
        "            best_val_r2 = val_r2\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {np.mean(train_losses[-len(train_loader):]):.4f}\"\n",
        "              f\" | Val Loss: {val_loss:.4f} | Val R-squared: {val_r2:.4f}\")\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Function to evaluate model performance on a dataset\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        losses = []\n",
        "        y_trues = []\n",
        "        y_preds = []\n",
        "        for data in data_loader:\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, torch.zeros_like(outputs))  # Assuming labels not available\n",
        "            losses.append(loss.item())\n",
        "            y_trues.extend([0] * len(data))  # Assuming labels are 0 for now\n",
        "            y_preds.extend(outputs.squeeze(1).tolist())  # Squeeze output for R-squared calculation\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    val_r2 = r2_score(y_trues, y_preds)\n",
        "    return val_loss, val_r2\n"
      ],
      "metadata": {
        "id": "zX33is4aPMH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "YGlAt50qOI6i",
        "outputId": "66fae56a-8086-4509-a722-1b53423325c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1bcd436accc4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNightLightDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_train.hdf\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming train dataset in a separate file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNightLightDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3b857402e043>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, transform)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Read the HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnight_light_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F10'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming F10 holds night light intensity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "data_path = \"path/to/BlackMarble_NightLights.hdf\"  # Replace with your data path\n",
        "model = PovertyMapper()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_dataset = NightLightDataset(data_path[:-4] + \"_train.hdf\")  # Assuming train dataset in a separate file\n",
        "val_dataset = NightLightDataset(data_path)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "test_dataset = NightLightDataset(\"path/to/BlackMarble_NightLights_test.hdf\")  # Replace with test data path\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "trained_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_losses = []\n",
        "    y_trues = []\n",
        "    y_preds = []\n",
        "    for data in test_loader:\n",
        "        outputs = trained_model(data)\n",
        "        # Assuming labels not available for test data\n",
        "        loss = criterion(outputs, torch.zeros_like(outputs))  # Calculate loss for evaluation purposes\n",
        "        test_losses.append(loss.item())\n",
        "        y_trues.extend([0] * len(data))  # Placeholder for labels (if not available)\n",
        "        y_preds.extend(outputs.squeeze(1).tolist())  # Squeeze output\n",
        "\n",
        "test_loss = np.mean(test_losses)\n",
        "test_r2 = r2_score(y_trues, y_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test R-squared: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2"
      ],
      "metadata": {
        "id": "KH9fhjjicEia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Data Acquisition\n",
        "# Assuming data is stored in a CSV file\n",
        "nighttime_data = pd.read_csv('nighttime_data.csv')\n",
        "\n",
        "# Step 2: Data Preprocessing (if needed)\n",
        "# Perform any necessary data cleaning or filtering\n",
        "\n",
        "# Step 3: Data Analysis (if needed)\n",
        "# Calculate statistics or perform additional analysis\n",
        "\n",
        "# Step 4: Heatmap Generation\n",
        "# Assuming data contains latitude, longitude, and intensity columns\n",
        "heatmap_data = nighttime_data[['latitude', 'longitude', 'intensity']]\n",
        "\n",
        "# Step 5: Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set(style=\"white\")\n",
        "sns.kdeplot(\n",
        "    heatmap_data['longitude'],\n",
        "    heatmap_data['latitude'],\n",
        "    weights=heatmap_data['intensity'],\n",
        "    cmap=\"inferno\",\n",
        "    shade=True,\n",
        "    bw='silverman'\n",
        ")\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Nighttime Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zmMhtNoRPltd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}